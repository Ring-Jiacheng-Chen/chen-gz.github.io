{"pages":[],"posts":[{"title":"Buck Converter","text":"Buck Converter equations \\[ v_L = v_g - V \\approx V_g -V \\] \\[ i_c = i_L - \\frac{V}{R} = i_L - I_o \\approx I - \\frac{V}{R} \\] \\[ \\frac{dv_c}{dt} = \\frac{i_c}{C} \\approx \\frac{I-\\frac{V}{R}}{C} \\] \\[ \\frac{di_l}{dt} = \\frac{v_L}{L} \\approx \\frac{V_g-V}{L} \\] circuit buck converter relationship of input and output voltage The relationship between input voltage \\(V_g\\) and output voltage $ V_s$: \\[ V_{s}=\\frac{1}{T_{s}} \\int_{0}^{T_{s}} v_{s}(t) d t=D V_{g} \\] DC component of \\(v_s(t)\\) As illustrated in Fig. \\(2.2 .\\) the integral is given by the area under the curve, or \\(D T_{s} V_{g}\\) . The average value is therefore \\[ \\left\\langle v_{s}\\right\\rangle=\\frac{1}{T_{s}}\\left(D T_{s} V_{g}\\right)=D V_{g} \\] So the average value, or DC component, of \\(v_{s}(t)\\) is equal to the duty cycle times the DC input voltage \\(V_{g}\\). the ripple of inductor current The inductor voltage \\(v_{L}(t)\\) is then given by \\[ v_{L}=V_{x}-v(t) \\] The inductor voltage can be found by use of the definition \\[ v_{L}(t)=L \\frac{d i_{L}(t)}{d t} \\] Thus, during the first interval, when \\(v_{L}(t)\\) is approximately \\(\\left(V_{x}-V\\right),\\) the slope of the inductor current waveform is \\[ \\frac{d i_{L}(t)}{d t}=\\frac{v_{L}(t)}{L} \\approx \\frac{v_{g}-v}{L} \\] \\[ \\frac{d i_{L}(t)}{d t} \\approx-\\frac{V}{L} \\] the inductor current ripple \\(\\Delta i_{L}\\) since we know the slope of the inductor curring the first subinterval, and we also know the length of the first subinterval, we can calculate the ripple magnitude. The \\(i_{t}(t)\\) waveform is symmetrical about \\(I,\\) and hence during the first so the change in current increases by 2\\(\\Delta i_{L}\\) (since \\(\\Delta i_{1}\\) is the peak ripple, the peak-to-peak ripple is \\(2\\Delta i_{l}\\)). So the change in current, \\(2 \\Delta i_{L},\\) is equal to the slope (the applied inductor voltage divided by \\(L\\) ) times the length of the first subinterval \\(\\left(D T_{s}\\right)\\) : \\[ \\left(2 \\Delta i_{L}\\right)=\\left(\\frac{V_{g}-V}{L}\\right)\\left(D T_{\\mathrm{s}}\\right) \\] \\[ \\Delta i_{L}=\\frac{V_{g}-V}{2 L} D T_{\\mathrm{s}} \\] the Capacitor voltage ripple \\(\\Delta v_{C}\\) why can not use \\(\\frac{dv_c}{dt}\\) to calcutlate why need use \\(\\Delta i_L\\) to calculate the value of \\(\\Delta v_C\\) By the capacitor relation \\(Q=C V\\) \\[ q=C(2 \\Delta v) \\] \\[ q=\\frac{1}{2} \\Delta i_{L} \\frac{T_{s}}{2} \\] \\[ \\Delta v=\\frac{\\Delta i_{L} T_{s}}{8 C} \\] reduce peak to peak ripple The inductor value can be chosen such that a desired current ripple \\(\\Delta i_{L}\\) is attained. Solution of \\(Eq. (2.15)\\) for the inductance \\(L\\) yields \\[ L=\\frac{V_{k}-V}{2 \\Delta i_{L}} D T_{\\mathrm{s}} \\] fit continue conduction mode get \\(L_{min}\\) \\[ L=\\frac{V_{g}-V}{2 \\Delta i_{l}} D T_{\\mathrm{s}} \\]","link":"/2019/09/21/Power-Electric/Buck-Converter/"},{"title":"Buck Boost Converter","text":"Buck Boost Converter buck-boost converter revert output voltage Another converter, the buck-boost converter, can either increase or decrease the magnitude of the voltage, but the polarity is inverted. So with a positive input voltage, the ideal buck-boost converter can produce a negative output voltage of any magnitude. circuit why this is a buck-boost converter when mos on, the circuit at status 1: \\[ v_L = V_g \\\\ i_C = \\frac{v}{R} \\approx \\frac{V}{R} \\\\ \\frac{di_L}{dt} = \\frac{v_L}{L} \\approx \\frac{V_g}{L} \\\\ \\frac{dv_C}{dt} = \\frac{i_C}{C} = \\frac{v}{RC} \\approx \\frac{V}{RC} \\] when mos off, the circuit at status 2: \\[ v_L = v \\approx V \\\\ i_C = i_L + i_o \\approx I_L + I_o \\\\ \\frac{di_L}{dt} = \\frac{v_L}{L} = \\frac{v}{L} \\approx \\frac{V}{L} \\\\ \\frac{dv_C}{dt} = \\frac{i_C}{C} = \\frac{i_L + i_0}{C} \\approx \\frac{I_L+I_o}{C} \\] to find out the relationship between input voltage and output voltage equate \\(&lt;v_L&gt; = 0\\). \\[ \\frac{V_g}{L}DT_s + \\frac{V}{L}D'T_s = 0 \\] then \\[ V = -\\frac{V_g D }{D'} \\] from the formula, when \\(D&lt;D'\\) is a buck converter. Otherwise this is a boost converter. we can figure out that \\[ D = \\frac{V}{V-V_g} \\] set \\(&lt;i_C&gt; = 0\\) \\[ \\frac{V}{RC}DT_s + \\frac{I_L+I_o}{C}D'T_s = 0 \\] \\[ I_o = -I_L D' \\\\ I_L = -\\frac{I_o}{D'} = -\\frac{V}{RD'} \\] ripple to find \\(\\Delta i_L\\) notice that \\(\\frac{di_L}{dt}\\) so we can get that \\[ 2\\Delta i_L = \\frac{V_g}{L}DT_s \\] to find \\(\\Delta v\\) notice that \\(\\frac{dv_C}{dt}\\) and $v_C = v $ so we can get that \\[ 2\\Delta v_C = \\frac{V}{RC}DT_s \\\\ \\Delta v_C = \\frac{-V_g D^2 T_s} {2RC(1-D)} \\]","link":"/2019/09/21/Power-Electric/Buck-Boost-Converter/"},{"title":"SEPIC-Converter","text":"SEPIC-Converter Single-ended primary-inductor converter Principle when mos on, the circuit work at status 1 from the firgure we can see that we have 3 circle. \\[ v_{L1} = V_g \\\\ i_{C1} = -i_{L2} \\approx -I_{L2} \\\\ v_{L2} = v_{C1} \\approx V_{C1} \\\\ i_{C2} = -\\frac{v_o}{R} \\approx -\\frac{V_o}{R} \\] \\[ \\frac{di_{L_1}}{dt} = \\frac{v_{L1}}{L_1} = \\frac{V_g}{L_1} \\\\ \\frac{du_{C_1}}{dt} = \\frac{i_{C_1}}{C_1} = \\frac{-i_{L_2}}{C_1} \\approx \\frac{-I_{L_2}}{C_1} \\\\ \\frac{di_{L_2}}{dt} = \\frac{v_{L_2}}{L_2} = \\frac{v_{C_1}}{L_2} \\approx \\frac{V_{C_1}}{L_2} \\\\ \\frac{du_{C_2}}{dt} = \\frac{i_{C_2}}{C_2} = -\\frac{v_o}{RC_2} \\approx -\\frac{V_o}{RC_2} \\\\ \\] when mos off, the circuit work at status 2 from the figure we have equations: \\[ v_{L1} = V_g - v_{C_1} - v_o \\approx V_g - V_{C_1} - V_o\\\\ i_{C1} = i_{L_1} \\approx I_{L_1} \\\\ v_{L2} = -v_o \\approx -V_o \\\\ i_{C2} = i_{L_1} + i_{L_2} - \\frac{v_o}{R} \\approx I_{L_1} + I_{L_2} - \\frac{V_o}{R} \\] \\[ \\frac{di_{L_1}}{dt} = \\frac{V_g-v_{C_1}-v_o}{L_1} = \\frac{V_g-V_{C_1}-V_o}{L_1} \\\\ \\frac{du_{C_1}}{dt} = \\frac{i_{C_1}}{C_1} = \\frac{i_{L_1}}{C_1} \\approx \\frac{I_{L_1}}{C_1} \\\\ \\frac{di_{L_2}}{dt} = \\frac{v_{L_2}}{L_2} = \\frac{-v_0}{L_2} \\approx \\frac{-V_{o}}{L_2} \\\\ \\frac{du_{C_2}}{dt} = \\frac{i_{C_2}}{C_2} = \\frac{i_{L_1}-\\frac{v_o}{R}}{C_2} \\approx \\frac{I_{L_1}-\\frac{V_o}{R}}{C_2} \\] to set \\(&lt;v_{L_1}&gt; = 0\\) \\[ \\frac{V_g}{L_1}DT_s + \\frac{V_g - V_{C_1} - V_o}{L_1}D'T_s = 0 \\] to set \\(&lt;i_{C_1}&gt; = 0\\) \\[ \\frac{-I_{L_2}}{C_1}DT_s + \\frac{I_{L_1}}{C_1} D'T_s = 0 \\] to set $&lt;u_{L_2}&gt; = 0 $ \\[ \\frac{V_{C_1}}{L_2}DT_s - \\frac{V_o}{L_2}D'T_s = 0 \\] to set \\(&lt;i_{C_2}&gt; = 0\\) \\[ -\\frac{V_o}{RC_2}DT_s +\\frac{I_{L_1} + I_{L_2} -\\frac{V_o}{R}}{C_2}D'T_s = 0 \\] from \\(equation (7)\\) \\[ V_{C_1}D = V_o D' \\] from \\(euqation(5)\\) \\[ V_g D + (V_g - V_{C_1} - V_o)D' = 0 \\\\ V_g + (-\\frac{V_oD'}{D} - V_o)D' =0 \\\\ V_o = \\frac{V_g D}{D'} \\] from \\(equation(9)\\) \\[ V_o = \\frac{V_{C_1}D}{D'} \\] assiociate \\(equation(9),(11)\\) we get that \\[ V_g = V_{C_1} \\] from \\(equation(6)\\) \\[ -I_{L_2}D + I_{L_1}D' = 0 \\\\ I_{L_2} = \\frac{I_{L_1}D'}{D} \\] from \\(euqaiton (8)\\) \\[ -\\frac{V_o}{R} +(I_{L_1}+I_{L_2})D' = 0\\\\ I_{L_1}+I_{L_2} = \\frac{V_o}{RD'} = \\frac{V_gD}{RD'D'} \\\\ I_{L_1}+\\frac{I_{L_1}D'}{D} = \\frac{V_gD}{RD'D'} \\\\ \\frac{I_{L_1}}{D} = \\frac{V_gD}{RD'D'} \\\\ I_{L_1} = (\\frac{D}{D'})^2 \\frac{V_g}{R} \\] from \\(equation(13)\\) \\[ I_{L_2} =\\frac{D}{D'} \\frac{V_g}{R} \\] use \\(\\frac{du_{C_1}}{dt} = \\frac{i_{C_1}}{C_1} = \\frac{-i_{L_2}}{C_1} \\approx \\frac{-I_{L_2}}{C_1} \\\\\\) we can get that \\[ 2 \\Delta v_{C_1} = \\frac{-I_{L_2}}{C_1}DT_s \\\\ \\Delta v_{C_1} = \\frac{D^2}{D'}T_S\\frac{V_g}{2RC_1} \\] use \\(\\frac{du_{C_2}}{dt} = \\frac{i_{C_2}}{C_2} = -\\frac{v_o}{RC_2} \\approx -\\frac{V_o}{RC_2} \\\\\\) \\[ 2\\Delta v_o = \\frac{V_o}{RC_2}DT_s \\\\ \\Delta v_o = \\frac{D^2}{D'}T_s\\frac{V_g}{2RC_2} \\] use $ = = $ \\[ 2\\Delta i_{L_1} = \\frac{V_g}{L_1}DT_s \\\\ \\Delta i_{L_1} = \\frac{V_g}{2L_1}DT_s \\\\ \\] use \\(\\frac{di_{L_2}}{dt} = \\frac{v_{L_2}}{L_2} = \\frac{-v_0}{L_2} \\approx \\frac{-V_{o}}{L_2} \\\\\\) \\[ 2\\Delta i_{L_2} = \\frac{V_o}{L_2}D'T_s \\\\ \\Delta i_{L_2} = \\frac{V_gD}{2L_2}T_s \\\\ \\]","link":"/2019/09/21/Power-Electric/SEPIC-Converter/"},{"title":"Greatest Common Divisor","text":"Greatest Common Divisor(GCD) Here is another passage about greatest common divisor. Common Divisor mean if \\(a|b\\) and \\(a|c\\) , then \\(a\\) is common divisor of \\(b\\) and \\(c\\). Greatest Common Divisor is the Greatest one of all common divisor of \\(b\\) and \\(c\\). For example, 12 and 8 have two common divisor, 2 and 4. To find out greatest common divisor is one of basic questions in number theory. It look like easy to find out the greatest common divisor of two small numbers. But we need a method to find out the greatest common divisor as quickly as possible. If a divides b, then the relationship of \\(a\\) and \\(b\\) can be expressed as \\(a = bx + r\\). Thus the great common divisor of \\(a\\) and \\(b\\) equal to the great common divisor of \\(bx+r\\) and \\(b\\). Notice that \\(b\\) divides \\(bx\\). So great common divisor of \\(a\\) and \\(b\\) equal great common divisor of \\(b\\) and \\(r\\). Explain more if \\(c\\) is the great common divisor of \\(a\\) and \\(b\\), then \\(c|bx +r\\). Because of \\(c|b\\) thus \\(c|bx\\). we have \\(c|bx+r\\) so \\(c\\) must divided by \\(r\\). Here is a most important equation in this section \\[ gcd(a,b) = gcd(a \\mod b,b) \\] Considering some times \\(x\\) may be zero, that means \\(b\\) bigger than \\(a\\). When write a computer program using recursion skill, we need change the position of \\(a\\) and \\(b\\). Rewrite the equation in order to it can be used in computer program. \\[ gcd(a,b) = gcd(b,a \\mod b) \\] \\(a\\) and \\(b\\) is co-prime if we find out that the greatest common divisor of \\(a\\) and \\(b\\) is 1.","link":"/2019/09/21/Number-Theory/Greatest-Common-Divisor/"},{"title":"Foundation and Basic Concept","text":"note: most things are copied from book, not write by myself and this book can get from Google. I only copy important things to here and add some comments. symbols Duty cycle: D single pole double throw: SPDT. output voltage \\(v_{s}(t)\\). converter input voltage \\(V_{g}\\). The complement of the duty ratio, \\(D^{\\prime},\\) is defined as \\((1-D)\\). DC component of \\(v_s(t)\\): From Fourier analysis, we know that the DC component of \\(v_{s}(t)\\) is given by its average value \\(\\left\\langle v_{s}\\right\\rangle,\\) or \\[ \\left\\langle v_{s}\\right\\rangle=\\frac{1}{T_{s}} \\int_{0}^{T_{s}} v_{s}(t) d t \\] ## position of inductor The buck converter is just one of many possible switching converters. Two other commonly used converters, which perform different voltage conversion functions, are illustrated in Fig. \\(2.5 .\\) In the boost converter, the positions of the inductor and switch are reversed. balance rule for inductor voltage and capacitor current The principles of inductor volt-second balance and capacitor charge balance are derived; these can be used to solve for the inductor currents and capacitor voltages of switching converters. small-ripple and linear-ripple approximation So it is nearly always a good approximation to assume that the magnitude of the switching ripple is much smaller than the DC component: \\[ \\left|v_{\\text {ripple}}\\right|&lt;V \\] Therefore, the output voltage \\(v(t)\\) is well approximated by its DC component \\(V,\\) with the small ripple term \\(v_{ripple}(t)\\) neglected: \\[ v(t) \\approx V \\] ## inductor voltage and conductor current inductor voltage The inductor voltage can be found by use of the definition \\[ v_{L}(t)=L \\frac{d i_{L}(t)}{d t} \\] Capacitor current \\[ i_c(t) = C \\frac{du_c(t)}{dt} \\] The balance of inductor and capacitor Inductor voltage balance the principle of inductor volt-second balance. Given the defining relation of an inductor: \\[ v_{L}(t)=L \\frac{d i_{L}(t)}{d t} \\] Integration over one complete switching period, say from \\(t=0\\) to \\(T_{s}\\) , yields \\[ i_{L}\\left(T_{s}\\right)-i_{L}(0)=\\frac{1}{L} \\int_{0}^{T_{s}} v_{L}(t) d t \\] Therefore, in steady state the integral of the applied inductor voltage must be zero: \\[ 0=\\int_{0}^{T_{s}} v_{L}(t) d t \\] ### Capacitor current balance Similar arguments can be applied to capacitors. The defining equation of a capacitor is \\[ i_{C}(t)=C \\frac{d v_{C}(t)}{d t} \\] Integration of this equation over one switching period yields \\[ v_{C}\\left(T_{s}\\right)-v_{C}(0)=\\frac{1}{C} \\int_{0}^{T_{s}} i_{C}(t) d t \\] In steady state, the net change over one switching period of the capacitor voltage must be zero, so that the left-hand side of \\(Eq. (2.26)\\) is equal to zero. Therefore, in equilibrium the integral of the capacitor current over one switching period (having the dimensions of amp-seconds, or charge) should be zero. There is no net change in capacitor charge in steady state. An equivalent statement is This should be an intuitive result. If a DC current is applied to a capacitor, then the \\[ 0=\\frac{1}{T_{s}} \\int_{0}^{T_{s}} i_{c}(t) d t=\\left\\langle i_{c}\\right\\rangle \\] The average value, or DC component, of the capacitor current must be zero in equilibrium. continue conduction mode current must over zero, if not satisfied continue conduction mode, then when switch to status 2, two component will off! from continue conduction mode we can define \\(L_{min}\\) to satisfy continue conduction mode. M(D) of various converter","link":"/2019/09/21/Power-Electric/Foundation-and-Basic-Concept/"},{"title":"Linear Time-Invariant(LTI) Systems","text":"time invariant A system is said to be time invariant if for every state-input–output pair \\[ \\left.\\begin{array}{l}{\\mathbf{x}\\left(t_{0}\\right)} \\\\\\ {\\mathbf{u}(t), t \\geq t_{0}}\\end{array}\\right\\} \\rightarrow \\mathbf{y}(t), \\quad t \\geq t_{0} \\] and any \\(T\\), we have \\[ \\left. \\begin{array}{l}{\\mathbf{x}\\left(t_{0}+T\\right)} \\\\ {\\mathbf{u}(t-T), t \\geq t_{0}+T}\\end{array}\\right\\} \\rightarrow \\mathbf{y}(t-T), t \\geq t_{0}+T \\quad \\text { (time shifting) } \\] It means that if the initial state is shifted to time \\(t_0 + T\\) and the same input waveform is applied from \\(t_0 + T\\) instead of from \\(t_0\\) , then the output waveform will be the same except that it starts to appear from time \\(t_0 + T\\) . In other words, if the initial state and the input are the same, no matter at what time they are applied, the output waveform will always be the same. Therefore, for time-invariant systems, we can always assume, without loss of generality, that \\(t_0 = 0\\). If a system is not time invariant, it is said to be time varying. for a linear system causal condition is \\[ \\text { Causal } \\Longleftrightarrow g(t, \\tau)=0 \\text { for } t&lt;\\tau \\] so for LTI system that \\(g(t) = 0, t&lt;t_0\\).","link":"/2019/09/21/Linear-Control-System/Linear-Time-Invariant/"},{"title":"Basic Information","text":"The systems to be studied in this categories are limited to linear systems. Using the concept of linearity, that every linear system can be described by \\[ \\mathbf{y}(t)=\\int_{t_{0}}^{t} \\mathbf{G}(t, \\tau) \\mathbf{u}(\\tau) d \\tau \\] This equation describes the relationship between the input u and output y and is called the input–output or external description. If a linear system is lumped as well, then it can also be described by \\[ \\dot{\\mathbf{x}}(t) =\\mathbf{A}(t) \\mathbf{x}(t)+\\mathbf{B}(t) \\mathbf{u}(t) \\] \\[ \\mathbf{y}(t) =\\mathbf{C}(t) \\mathbf{x}(t)+\\mathbf{D}(t) \\mathbf{u}(t) \\] \\(Equation (2)\\) is a set of ﬁrst-order differential equations and \\(Equation (3)\\) is a set of algebraic equations. They are called the internal description of linear systems. Because the vector \\(x\\) is called the state, the set of two equations, \\(equation (2)\\) and \\(equation (3)\\), is called the state-space or, simply, the state equation. If a linear system has, in addition, the property of time invariance, then \\(Equations (1)\\) through \\((3)\\) reduce to \\[ \\mathbf{y}(t)=\\int_{0}^{t} \\mathbf{G}(t-\\tau) \\mathbf{u}(\\tau) d \\tau \\] and \\[ \\begin{array}{c} {\\dot{\\mathbf{x}}(t)=\\mathbf{A} \\mathbf{x}(t)+\\mathbf{B u}(t)} \\\\ {\\mathbf{y}(t)=\\mathbf{C x}(t)+\\mathbf{D} \\mathbf{u}(t)}\\end{array} \\] For this class of linear time-invariant systems, the Laplace transform is an important tool in analysis and design. Applying the Laplace transform to (1.4) yields \\[ \\hat{\\mathbf{y}}(s)=\\hat{\\mathbf{G}}(s) \\hat{\\mathbf{u}}(s) \\] where a variable with a circumﬂex denotes the Laplace transform of the variable. The function \\(\\hat{G}(s)\\) is called the transfer matrix. Both \\((4)\\) and \\((6)\\) are input–output or external descriptions. The former is said to be in the time domain and the latter in the frequency domain. Categories SISO A system with only one input terminal and only one output terminal is called a single-variable system or a single-input single-output (SISO) system. MIMO A system with two or more input terminals and/or two or more output terminals is called a multi-variable system. More speciﬁcally, we can call a system a multi-input multi-output (MIMO) system if it has two or more input terminals and output terminals. SIMO A single-input multi-output (SIMO) system if it has one input terminal and two or more output terminals. continuous-time A system is called a continuous-time system if it accepts continuous-time signals as its input and generates continuous-time signals as its output. discrete-time A system is called a discrete-time system if it accepts discrete-time signals as its input and generates discrete-time signals as its output. memoryless system A system is called a memoryless system if its output y(t 0 ) depends only on the input applied at t 0 ; it is independent of the input applied before or after t 0 . This will be stated succinctly as follows: current output of a memoryless system depends only on current input; it is independent of past and future inputs. causal or non-anticipatory A system is called a causal or non-anticipatory system if its current output depends on past and current inputs but not on future input. If a system is not causal, then its current output will depend on future input. In other words, a non-causal system can predict or anticipate what will be applied in the future. No physical system has such capability. Therefore every physical system is causal and causality is a necessary condition for a system to be built or implemented in the real world. This text studies only causal systems. Using the state at \\(t_0\\) , we can express the input and output of a system as \\[ \\left. \\begin{array}{l}{\\mathbf{x}\\left(t_{0}\\right)} \\\\ {\\mathbf{u}(t), t \\geq t_{0}}\\end{array} \\right\\} \\rightarrow \\mathbf{y}(t), \\quad t \\geq t_{0} \\] It means that the output is partly excited by the initial state at \\(t_0\\) and partly by the input applied at and after \\(t_0\\) . In using \\(definition(2.1)\\), there is no more need to know the input applied before \\(t_0\\) all the way back to \\(\\infin\\). Thus \\(definition(2.1)\\) is easier to track and will be called a state-input–output pair. lumped A system is said to be lumped if its number of state variables is ﬁnite or its state is a ﬁnite vector. distributed A system is called a distributed system if its state has inﬁnitely many state variables. The transmission line is the most well known distributed system. linear system additivity, homogeneity and superposition A system is called a linear system if for every \\(t_0\\) and any two state-input–output pairs \\[ \\left. \\begin{array}{l}{\\mathbf{x}\\left(t_{0}\\right)} \\\\ {\\mathbf{u}(t), t \\geq t_{0}}\\end{array}\\right\\} \\rightarrow \\mathbf{y}(t), \\quad t \\geq t_{0} \\] for \\(i =1,2\\), we have \\[ \\left. \\begin{array}{rl}{\\mathbf{x}_{1}\\left(t_{0}\\right)+\\mathbf{x}_{2}\\left(t_{0}\\right)} &amp; {} \\\\ {\\mathbf{u}_{1}(t)+\\mathbf{u}_{2}(t),} &amp; {t \\geq t_{0}}\\end{array}\\right\\} \\rightarrow \\mathbf{y}_{1}(t)+\\mathbf{y}_{2}(t), \\quad t \\geq t_{0} \\text { (additivity) } \\] and \\[ \\left. \\begin{array}{l}{\\alpha \\mathbf{x}_{1}\\left(t_{0}\\right)} \\\\ {\\alpha \\mathbf{u}_{1}(t), \\quad t \\geq t_{0}}\\end{array} \\right\\} \\rightarrow \\alpha \\mathbf{y}_{1}(t), \\quad t \\geq t_{0} \\text { (homogeneity) } \\] for any real constant \\(\\alpha\\). The ﬁrst property is called the additivity property, the second, the homogeneity property. These two properties can be combined as \\[ \\left. \\begin{array}{l}{\\alpha_{1} \\mathbf{x}_{1}\\left(t_{0}\\right)+\\alpha_{2} \\mathbf{x}_{2}\\left(t_{0}\\right)} \\\\ {\\alpha_{1} \\mathbf{u}_{1}(t)+\\alpha_{2} \\mathbf{u}_{2}(t), \\quad t \\geq t_{0}}\\end{array} \\right \\} \\rightarrow \\alpha_{1} \\mathbf{y}_{1}(t)+\\alpha_{2} \\mathbf{y}_{2}(t), \\quad t \\geq t_{0} \\] for any real constants \\(\\alpha_1\\) and \\(\\alpha_2\\), and is called the superposition property. A system is called a nonlinear system if the superposition property does not hold. zero input response If the input \\(\\mathbf{u}(t)\\) is identically zero for \\(t \\geq t_{0}\\) , then the output will be excited exclusively by the initial state \\(\\mathbf{x}\\left(t_{0}\\right).\\) This output is called the zero-input response and will be denoted by\\(\\mathbf{y}_{z i}\\) or \\[ \\left. \\begin{array}{l}{\\mathbf{x}\\left(t_{0}\\right)} \\\\ {\\mathbf{u}(t) \\equiv \\mathbf{0}, t \\geq t_{0}}\\end{array}\\right \\}\\rightarrow \\mathbf{y}_{z i}(t), \\quad t \\geq t_{0} \\] zero state response If the initial state \\(\\mathbf{x}\\left(t_{0}\\right)\\) is zero, then the output will be excited exclusively by the input. This output is called the zero-state response and will be denoted by \\(\\mathbf{y}_{z s}\\) or \\[ \\left. \\begin{array}{rl}{\\mathbf{x}\\left(t_{0}\\right)=} {\\mathbf{0}} \\\\ {\\mathbf{u}(t),} 4 {t \\geq t_{0}}\\end{array}\\right\\} \\rightarrow \\mathbf{y}_{z s}(t), \\quad t \\geq t_{0} \\]","link":"/2019/09/21/Linear-Control-System/Basic-Information/"},{"title":"Introduce Number Theory","text":"preface In the past hundred years, people thing number theory is the queen of math(math is the queen of science). we find it very interesting. In the number theory we can find many many amazing things. The conclusion is very short and very very beautify. But almost all of them are very hard to prove. This is very interesting things in the nature. Number theory is from integers. When we talk about number theory, always focus on positive not negative. We can find that first amazing thing of number theory is we only have integers. the most beautiful and easily thing in the world. In this passage will introduce some interest and basic information about number theory. I hope first passage of number theory is attractive and easy to read. prime number prime number A prime number (or a prime) is a natural number greater than 1 that cannot be formed by multiplying two smaller natural numbers[from wikipedia]. Such as 2,3,5,7,11,13,17 is prime number. Almost all problem of number theory is about prime number. Goldbach conjecture and most famous unsolved problem. We call............... twin prime like 3,5 or 11,13. We wander to know if the prime is very large, does there exist twin prime and the frequency of twin number. there all very interesting things.","link":"/2019/09/21/Number-Theory/Introduce-Number-Theory/"},{"title":"Contents","text":"Basici Information Input Output Description Linear Time Invariant(LTI) system","link":"/2019/09/21/Linear-Control-System/Contents/"},{"title":"input-output description","text":"Consequently, the input u(t) can be expressed symbolically as \\[ u(t) \\approx \\sum_{i} u\\left(t_{i}\\right) \\delta_{\\Delta}\\left(t-t_{i}\\right) \\Delta \\] Let \\(g_{\\Delta}\\left(t, t_{i}\\right)\\) be the output at time \\(t\\) excited by the pulse \\(u(t)=\\delta_{\\Delta}\\left(t-t_{i}\\right)\\) applied at time \\(t_{i}\\). Then we have \\[ \\begin{aligned} \\delta_{\\Delta}\\left(t-t_{i}\\right) &amp; \\rightarrow g_{\\Delta}\\left(t, t_{i}\\right) \\\\ \\delta_{\\Delta}\\left(t-t_{i}\\right) u\\left(t_{i}\\right) \\Delta &amp; \\rightarrow g_{\\Delta}\\left(t, t_{i}\\right) u\\left(t_{i}\\right) \\Delta \\quad \\text { (homogeneity) } \\\\ \\sum_{i} \\delta_{\\Delta}\\left(t-t_{i}\\right) u\\left(t_{i}\\right) \\Delta &amp; \\rightarrow \\sum_{i} g_{\\Delta}\\left(t, t_{i}\\right) u\\left(t_{i}\\right) \\Delta \\quad \\text { (additivity) } \\end{aligned} \\] Thus the output \\(y(t)\\) excited by the input \\(u(t)\\) can be approximated by \\[ y(t) \\approx \\sum_{i} g_{\\Delta}\\left(t, t_{i}\\right) u\\left(t_{i}\\right) \\Delta \\] Now if \\(\\Delta\\) approaches zero, the pulse \\(\\delta_{\\Delta}\\left(t-t_{i}\\right)\\) becomes an impulse at \\(t_{i},\\) denoted by \\(\\delta\\left(t-t_{i}\\right),\\) and the corresponding output will be denoted by \\(g\\left(t, t_{i}\\right) .\\) As \\(\\Delta\\) approaches zero, the approximation in \\(equation (2)\\) becomes an equality, the summation becomes an integration, the discrete \\(t_{i}\\) becomes a continuum and can be replaced by \\(\\tau,\\) and \\(\\Delta\\) can be written as \\(d \\tau .\\) Thus \\((2)\\) becomes \\[ y(t)=\\int_{-\\infty}^{\\infty} g(t, \\tau) u(\\tau) d \\tau \\] Note that \\(g(t, \\tau)\\) is a function of two variables. The second variable denotes the time at which the impulse input is applied; the ﬁrst variable denotes the time at which the output is observed. Because \\(g(t,\\tau)\\) is the response excited by an impulse, it is called the impulse response. If a system is causal, the output will not appear before an input is applied. Thus we have \\[ \\text { Causal } \\Longleftrightarrow g(t, \\tau)=0 \\text { for } t&lt;\\tau \\] relax A system is said to be relaxed at \\(t_0\\) if its initial state at \\(t_0\\) is 0. In this case, the output \\(y(t)\\), for \\(t ≥ t_0\\) , is excited exclusively by the input \\(u(t)\\) for \\(t \\geq 0\\). In conclusion, every linear system that is causal and relaxed at \\(t_0\\) can be described by \\[ \\mathbf{G}(t, \\tau)=\\left[\\begin{array}{cccc}{g_{11}(t, \\tau)} &amp; {g_{12}(t, \\tau)} &amp; {\\cdots} &amp; {g_{1 p}(t, \\tau)} \\\\\\ {g_{21}(t, \\tau)} &amp; {g_{22}(t, \\tau)} &amp; {\\cdots} &amp; {g_{2 p}(t, \\tau)} \\\\\\ {\\vdots} &amp; {\\vdots} &amp; {} &amp; {\\vdots} \\\\\\ {g_{q 1}(t, \\tau)} &amp; {g_{q 2}(t, \\tau)} &amp; {\\cdots} &amp; {g_{q p}(t, \\tau)}\\end{array}\\right] \\] where \\[ \\mathbf{y}(t)=\\int_{t_{0}}^{t} \\mathbf{G}(t, \\tau) \\mathbf{u}(\\tau) d \\tau \\] and \\(g_{ij}(t,\\tau)\\) is the response at time \\(t\\) at the \\(i\\)th output terminal due to an impulse applied at time \\(\\tau\\) at the \\(j\\) th input terminal, the inputs at other terminals being identically zero. That is, \\(g_{ij}(t, \\tau )\\) is the impulse response between the \\(j\\) th input terminal and the \\(i\\)th output terminal. Thus \\(G\\) is called the impulse response matrix of the system.","link":"/2019/09/21/Linear-Control-System/Input-Output-Description/"},{"title":"Sample Space and Event","text":"The Sample Space outcome or sample point We define an outcome or sample point of a random experiment as a result that cannot be decomposed into other results. When we perform a random experiment, one and only one outcome occurs. The sample space \\(S\\) of a random experiment is defined as the set of all possible outcomes. We call \\(S\\) a discrete sample space if \\(S\\) is countable; that is, its outcomes can be put into one-to-one correspondence with the positive integers. We call \\(S\\) a continuous sample space if \\(S\\) is not countable. discrete sample space Discrete sample space means that the element in the sample space are sperate. Discrete sample space has countable elelment in the sample space. The number of sample space can be finite and infinite. notice that if a set have infinite element, it can be countable, not always uncountable. continue sample space Continuous sample spaces arise in experiments in which the outcomes are numbers that can assume a continuum of values, so we let the sample space \\(S\\) be the entire real line \\(R\\) (or some interval of the real line). Events Events correspond to subsets of \\(S\\)(sample space). correspond to subsets of \\(S\\). Two events of special interest are the certain event, \\(S\\), which consists of all outcomes and hence always occurs, and the impossible or null event, \\(\\emptyset\\), which contains no outcomes and hence never occurs. An event from a discrete sample space that consists of a single outcome is called an elementary event.","link":"/2019/09/21/Probability-and-Random-Processes/Sample-Space-and-Event/"},{"title":"Probability Properties","text":"Conditional Probability Quite often we are interested in determining whether two events, \\(A\\) and \\(B\\) , are related in the sense that knowledge about the occurrence of one, say \\(B\\) , alters the likelihood of occurrence of the other, \\(A\\) . This requires that we find the conditional probability, \\(P[A | B],\\)of event \\(A\\) given that event \\(B\\) has occurred. The conditional probability is defined by \\[ P[A | B]=\\frac{P[A \\cap B]}{P[B]} \\quad \\text { for } P[B]&gt;0 \\] If we multiply both sides of the definition of \\(P[A | B]\\) by \\(P[B]\\) we obtain \\[ P[A \\cap B]=P[A | B] P[B] \\] Similarly we also have that \\[ P[A \\cap B]=P[B | A] P[A] . \\] Total Probability Let \\(B_{1}, B_{2}, \\ldots, B_{n}\\) be mutually exclusive events whose union equals the sample space \\(S\\). We refer to these sets as a partition of \\(S\\) . Any event \\(A\\) can be represented as the union of mutually exclusive events in the following way: \\[ \\begin{aligned} A=&amp; A \\cap S=A \\cap\\left(B_{1} \\cup B_{2} \\cup \\cdots \\cup B_{n}\\right) \\\\ &amp;=\\left(A \\cap B_{1}\\right) \\cup\\left(A \\cap B_{2}\\right) \\cup \\cdots \\cup\\left(A \\cap B_{n}\\right) \\end{aligned} \\] By applying \\(Eq(2.28 a)\\) to each of the terms on the right-hand side, we obtain the \\[ \\begin{aligned} P[A]=P\\left[A | B_{1}\\right] P\\left[B_{1}\\right]+P\\left[A | B_{2}\\right] P\\left[B_{2}\\right]+\\cdots+P\\left[A | B_{n}\\right] P\\left[B_{n}\\right] \\end{aligned} \\] Bayes' Rule Let \\(B_{1}, B_{2}, \\ldots, B_{n}\\) be a partition of a sample space \\(S\\) . Suppose that event \\(A\\) occurs; what is the probability of event \\(B_{j} ?\\) By the definition of conditional probability we have \\[ P\\left[B_{j} | A\\right]=\\frac{P\\left[A \\cap B_{j}\\right]}{P[A]}=\\frac{P\\left[A | B_{j}\\right] P\\left[B_{j}\\right]}{\\sum_{k=1}^{n} P\\left[A | B_{k}\\right] P\\left[B_{k}\\right]} \\] where we used the theorem on total probability to replace \\(P[A] .\\) Equation \\((2)\\) is called Bayes' rule. Independence of Events We will define two events \\(A\\) and \\(B\\) to be independent if \\[ P[A \\cap B]=P[A] P[B] \\] Equation (2.31) then implies both \\[ P[A | B]=P[A] \\] and \\[ P[B | A]=P[B] \\] Bernoulli trial A Bernoulli trial involves performing an experiment once and noting whether a particular event \\(A\\) occurs. The outcome of The Bernoulli trial is said to be a success if A occurs and a &quot;failure&quot; otherwise. In this section we are interested in finding the probability of \\(k\\) successes in \\(n\\) dependent repetitions of a Bernoulli trail. Condition Disjoint Probability Mass function Many random experiments have natural ways of partitioning the sample space \\(S\\) into the union of disjoint events \\(B_1, B_2,...,B_n\\). Let \\(p_X(x|B_i)\\) be the conditional pmf of \\(X\\) given event \\(B_i\\). The theorem on total probability allows us to find the pmf of \\(X\\) in terms of the conditional pmf’s: \\[ p_{X}(x)=\\sum_{i=1}^{n} p_{X}\\left(x | B_{i}\\right) P\\left[B_{i}\\right] \\]","link":"/2019/09/21/Probability-and-Random-Processes/Conditional-Probability/"},{"title":"SequenceTrail Laws","text":"Multinomial Coefficient Suppose we partition a set of \\(n\\) distinct objects into \\(\\mathcal{J}\\) subsets \\(B_{1}, B_{2}, \\ldots, B_{\\mathcal{J}},\\) where \\(B_{\\mathcal{J}}\\) is assigned \\(k_{\\mathcal{J}}\\) elements and \\(k_{1}+k_{2}+\\cdots+k_{\\mathcal{J}}=n .\\) In Problem \\(2.61,\\) it is shown that the number of distinct partitions is \\[ \\frac{n !}{k_{1} ! k_{2} ! \\ldots k_{J} !} \\] \\(Equation 1\\)is called the multinomial coefficient. The binomial coefficient is the \\(\\mathcal{J}=2\\) case of the multinomial coefficient. Binomial Probability Law Let \\(k\\) be the number of successes in \\(n\\) independent Bernoulli trials, then the probabilities of \\(k\\) are given by the binomial probability law: \\[ p_{n}(k)=\\left(\\begin{array}{l}{n} \\\\ {k}\\end{array}\\right) p^{k}(1-p)^{n-k} \\quad$ for $\\quad k=0, \\ldots, n \\] multinomial probability law Suppose that \\(n\\) independent repetitions of the experiment are performed. Let \\(k_{j}\\) be the number of times event \\(B_{j}\\) occurs, then the vector \\(\\left(k_{1}, k_{2}, \\ldots, k_{M}\\right)\\) specifies the number of times each of the events \\(B_{j}\\) occurs. The probability of the vector \\(\\left(k_{1}, \\ldots, k_{M}\\right)\\) satisfies the multinomial probability law: \\[ P\\left[\\left(k_{1}, k_{2}, \\ldots, k_{M}\\right)\\right]=\\frac{n !}{k_{1} ! k_{2} ! \\ldots k_{M} !} p_{1}^{k_{1}} p_{2}^{k_{2}} \\cdots p_{M}^{k_{M}} \\] where \\(k_{1}+k_{2}+\\cdots+k_{M}=n .\\) The binomial probability law is the \\(M=2\\) case of the multinomial probability law. The Geometric Probability Law Consider a sequential experiment in which we repeat independent Bernoulli trials until the occurrence of the first success. The probability of this event is \\[ p(m)=P\\left[A_{1}^{c} A_{2}^{c} \\ldots A_{m-1}^{c} A_{m}\\right]=(1-p)^{m-1} p \\quad m=1,2, \\ldots \\]","link":"/2019/09/21/Probability-and-Random-Processes/Sequence-Trail-Laws/"},{"title":"Preface","text":"This branch contains the Theorems, Lemmas and Corollaries in the book.","link":"/2019/09/21/Ring_C/Number_Theory/Book/Chapter0_Preface/"},{"title":"Divisibility","text":"1.1 Divisors Theorem 1.1 If \\(a\\) and \\(b\\) are integers with \\(b&gt;0\\), then there is a unique pair of integers \\(q\\) and \\(r\\) such that \\[ a = qb+r \\quad and \\quad 0 \\leq r &lt; b. \\] Corollary 1.2 If \\(a\\) and \\(b\\) are integers with \\(b \\neq 0\\), then there is a unique pair of integers \\(q\\) and \\(r\\) such that \\[ a = qb + r \\quad and \\quad 0 \\leq r &lt; |b| \\] Theorem 1.3 If \\(c\\) divides \\(a_1,...,a_k\\), then \\(c\\) divides \\(a_1u_1 + ... + a_ku_k\\) for all integers \\(u_1,...,u_k\\). \\(a|b\\) and \\(b|a\\) if and only if \\(a = \\pm b\\). Corollary 1.4 If \\(c\\) divides \\(a\\) and \\(b\\), then \\(c\\) divides \\(au+bv\\) for all integers \\(u\\) and \\(v\\). Definition If \\(d|a\\) and \\(d|b\\) we say that \\(d\\) is a \\(common \\ divisor\\) (or \\(common \\ factor\\)) of \\(a\\) and \\(b\\); for instance, \\(1\\) is a common divisor of any pair of integers \\(a\\) and \\(b\\). if \\(a\\) and \\(b\\) are not both \\(0\\), then no common divisor is greater than max(\\(|a|\\),\\(|b|\\)), so that among all their common divisors there is a greatest one. This is the \\(greatest \\ common \\ divisor\\) (or \\(highest \\ common \\ factor\\)) of \\(a\\) and \\(b\\); it is the unique integer \\(d\\) satisfying \\(d|a\\) and \\(d|b\\) (so that \\(d\\) is a common divisor), if \\(c|a\\) and \\(c|b\\) then \\(c \\leq d\\) (so that no common divisor exceeds \\(d\\)). Lemma 1.5 If \\(a = qb + r\\) then gcd(\\(a\\),\\(b\\)) = gcd(\\(b\\),\\(r\\)). Theorem 1.6 By [Euclidean Algorithm], we have \\(d = r_n-1\\) (the last non-zero remainder).","link":"/2019/09/21/Ring_C/Number_Theory/Book/Chapter1_Divisibility/"},{"title":"Euclidean Algorithm","text":"By Corollary 1.4, any common divisor of \\(b\\) and \\(r\\) also divides \\(qb+r=a\\); sinilarly, since \\(r = a - qb\\), it follows that any common divisor of \\(a\\) and \\(b\\) also divides \\(r\\). Thus the two pairs \\(a\\), \\(b\\) and \\(b\\), \\(r\\) have the same common divisors, so they have the same greatest common divisor.","link":"/2019/09/21/Ring_C/Number_Theory/Book/Euclidean_Algorithm/"},{"title":"Probability Functions","text":"Probability Density Function (PDF) Continuous random variables are characterized by probability density function(PDF). The probability mass function (pmf) of a discrete random variable \\(X\\) is defined as: \\[ p_{X}(x)=P[X=x]=\\mathrm{P}[\\{\\zeta : X(\\zeta)=x\\}] \\text { for } x \\text { a real number. } \\] property \\(p_{X}(x) \\geq 0\\) for all \\(x\\) \\(\\sum_{x \\in S_{X}} p_{X}(x)=\\sum_{\\text {all } k} p_{X}\\left(x_{k}\\right)=\\sum_{\\text {all } k} P\\left[A_{k}\\right]=1\\) \\(P[X \\text { in } B]=\\sum_{x \\in B} p_{X}(x)\\) where \\(B \\subset S_{X}\\) Cumulative Distribution Function(CDF) Conditional Probability Mass Function The conditional probability mass function of \\(X\\) is defined by the conditional probability: \\[ p_{X}(x | C)=P[X=x | C] \\quad \\text{ for $x$ a real number.} \\] Applying the definition of conditional probability we have: \\[ p_{X}(x | C)=\\frac{P[\\{X=x\\} \\cap C]}{P[C]} \\]","link":"/2019/09/21/Probability-and-Random-Processes/Probability-Functions/"},{"title":"Propety of Probability","text":"Basic propety \\(A\\) is event, \\(S\\) is sample space. \\(0 \\leq P[A]\\) \\(P[S]=1\\) If \\(A \\cap B=\\varnothing,\\) then \\(P[A \\cup B]=P[A]+P[B]\\) \\(P\\left[A^{c}\\right]=1-P[A]\\) \\(P[A] \\leq 1\\) \\(P[\\varnothing]=0\\) If \\(A_{1}, A_{2}, \\ldots, A_{n}\\) are pairwise mutually exclusive, then \\[ P\\left[\\bigcup_{k=1}^{n} A_{k}\\right]=\\sum_{k=1}^{n} P\\left[A_{k}\\right] \\quad \\text { for } n \\geq 2 \\] \\(P[A \\cup B]=P[A]+P[B]-P[A \\cap B]\\) Expected Value or Mean Value The expected value or mean of a discrete random variable \\(X\\) is defined by \\[ m_{X}=E[X]=\\sum_{x \\in S_{X}} x p_{X}(x)=\\sum_{k} x_{k} p_{X}\\left(x_{k}\\right) \\] Given a random variable \\(X\\) with probability mass function \\(p_{X}(x)\\) and a random variable \\(Y=g(X)\\) which is a function of \\(X .\\) We can compute the expected value of \\(Y\\) without deriving its probability mass function: \\[ E[Y]=\\sum_{x} g(x) p_{X}(x) \\] \\(n\\)th Moment The \\(n^{\\text {th }}\\) moment of a random variable \\(X\\) is \\[ \\mu_{X}=E\\left[X^{n}\\right]=\\sum_{x} x^{n} p_{X}(x) \\] \\(n\\)th Central Moment The \\(n^{\\text {th } \\text { central moment of a random variable } X \\text { is }}\\) \\[ E\\left[\\left(X-\\mu_{X}\\right)^{n}\\right]=\\sum_{x}\\left(x-\\mu_{X}\\right)^{n} p_{X}(x) \\] Variance Value The variance of a random variable \\(X :\\) \\[ \\operatorname{Var}[X]=E\\left[(X-E[X])^{2}\\right]=E\\left[\\left(X-\\mu_{X}\\right)^{2}\\right] \\] \\[ \\operatorname{Var}[X]=\\sum_{x}\\left(x-\\mu_{X}\\right)^{2} p_{X}(x) \\] \\[ \\operatorname{Var}[X]=E\\left[X^{2}\\right]-\\mu_{X}^{2} \\] Linear Operation Given a random variable \\(X\\) with probability mass function \\(p_{X}(x),\\) and a random variable \\(Y=a X+b\\) is a linear function of \\(X(a \\text { and } b \\text { are constants) then }\\) \\[ E[Y]= E[a X+b]=a E[X]+b \\\\ Var[Y]=\\operatorname{Var}[a X+b]=a^{2} \\operatorname{Var}[X] \\] ## Standard Deviantion The standard deviation of a random variable \\(X :\\) \\[ \\sigma_{X}=\\sqrt{\\operatorname{Var}[X]} \\] Conditional Expected Value Let \\(X\\) be a discrete random variable, and suppose that we know that event \\(B\\) has occurred. The conditional expected value of \\(X\\) given \\(B\\) is defined as: \\[ m_{X | B}=E[X | B]=\\sum_{x \\in S_{X}} x p_{X}(x | B)=\\sum_{k} x_{k} p_{X}\\left(x_{k} | B\\right) \\] where we apply the absolute convergence requirement on the summation. The conditional variance of \\(X\\) given \\(B\\) is defined as: \\[ \\begin{aligned} \\operatorname{VAR}[X | B] &amp;=E\\left[\\left(X-m_{X | B}\\right)^{2} | B\\right]=\\sum_{k=1}^{\\infty}\\left(x_{k}-m_{X | B}\\right)^{2} p_{X}\\left(x_{k} | B\\right) \\\\ &amp;=E\\left[X^{2} | B\\right]-m_{X | B}^{2} \\end{aligned} \\] Note that the variation is measured with respect to \\(m_{X | B},\\) not \\(m_{X}\\).","link":"/2019/09/21/Probability-and-Random-Processes/Propety-of-Probability/"},{"title":"Type of Continuous Random Variable","text":"Exponential Random Variable Erlang Random Variable Continuous version of the discrete Pascal Random Variable. Gamma Random Variable Beta Random Variable Gaussian Random Variable Also called the normal random variable because of its prevalence. \\(X \\sim\\) Gaussian \\((\\mu, \\sigma) :\\) \\[ f_{X}(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} e^{-(x-\\mu)^{2} / 2 \\sigma^{2}} \\]","link":"/2019/09/21/Probability-and-Random-Processes/Type-of-Continuous-Random-Variable/"},{"title":"Type of Discrete Random Variable","text":"Uniform Random Variable For each \\(k\\) in \\(S_{X},\\) we have \\(p_{X}(k)=1 / M\\). \\(X \\sim\\) Uniform \\((a, b),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{b-a+1}} &amp; {\\text { if } x=a, a+1, \\cdots, b} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] \\(E[X] = \\frac{a+b}{2}\\) \\(\\operatorname{Var}[X]=\\frac{(b-a+1)^{2}-1}{12}=\\frac{(b-a)(b-a+2)}{12}\\) Bernoulli Random Variable Used for experiments with two outcomes of interest: The success or failure of an experiment \\(X \\sim\\) Bernoulli \\((p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{1-p} &amp; {\\text { if } x=0} \\\\ {p} &amp; {\\text { if } x=1} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] * \\(S_X = \\{0,1\\}\\) * Mean: \\(E[X] = p\\) * \\(VAR[X]=p(1-p)\\) Binomial Random Variable Used for experiments that involve n independent trials of the Bernoulli experiment: \\(X \\sim\\) Binomial \\((n, p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\left(\\begin{array}{l}{n} \\\\ {x}\\end{array}\\right) p^{x}(1-p)^{n-x}} &amp; {\\text { if } x=\\{0,1, \\cdots, n\\}} \\\\ {0} &amp; {\\text { otherwise. }}\\end{array}\\right. \\] Mean: \\(E[X] = np\\) Variance: \\(Var[X] = np(1-p)\\) Geometric Random Variable $ X $ Geometric \\((p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{p(1-p)^{x-1}} &amp; {\\text { if } x=1,2,3, \\cdots} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] Mean: \\(E[x] = \\frac{1}{p}\\) Variance: \\(Var[X] = \\frac{1-p}{p^2}\\) notice that if \\(x\\) begin from \\(0\\), the mean is \\(E(x) = \\frac{1-p}{p}\\). Pascal Random Variable(Negtive binomial Distribution) For an infinite sequence of Benoulli trials each with probability \\(p\\), the Pascal random variable is the number of trials up to and including the \\(k\\)th success. To derive the Pascal \\(PMF\\), we consider the scenario of finding the \\(k\\) th success in the \\(x\\) th trial: there are exactly \\((k-1)\\) successes in the previous \\((x-1)\\) trials, and the \\(k\\) th success occurs in the \\(x\\) th trial. The probability of this event is: \\[ P\\{X=x\\}=\\left(\\begin{array}{l}{x-1} \\\\ {k-1}\\end{array}\\right) p^{k-1}(1-p)^{(x-1)-(k-1)} \\cdot p \\] \\(X \\sim\\) Pascal \\((k, p),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\left(\\begin{array}{c}{x-1} \\\\ {k-1}\\end{array}\\right) p^{k}(1-p)^{(x-1)-(k-1)} \\text { if } x=k, k+1, k+2, \\cdots} \\\\ {0} &amp; {\\text { otherwise. }}\\end{array}\\right. \\] Mean: \\(E[X]=\\frac{k}{p}\\) Variance: \\(\\operatorname{Var}[X]=\\frac{k(1-p)}{p^{2}}\\). Poisson Random Variable Used to describe phenomenon that occur randomly in time. While the time of each occurrence is completely random, there is a known average number of occurrences per unit time. In many applications, we are interested in counting the number of occurrences of an event in a certain time period or in a certain region in space. The Poisson random variable arises in situations where the events occur “completely at random” in time or space. \\(X \\sim\\) Poisson \\((\\alpha),\\) if its PMF has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{\\alpha^{x}}{x !} e^{-\\alpha}} &amp; {\\text { if } x=0,1,2, \\cdots} \\\\ {0} &amp; {\\text { otherwise. }}\\end{array}\\right. \\] where the parameter \\(\\alpha&gt;0\\). When dealing with problems involving time \\(t\\) with an arrival rate of \\(\\lambda\\) for an event, the Poisson probability of \\(x\\) arrivals in time \\(t\\) is found by setting the Poisson parameter \\(\\alpha=\\lambda t\\) \\(E[X] = \\alpha\\) \\(Var[X] = \\alpha\\) Zipf Random Variable Used to account for the relative popularity of a few members of a population and the relative uncertainty of other members of a population: \\(X \\sim \\operatorname{Zipf}(\\alpha, L),\\) if its \\(PMF\\) has the following form: \\[ p_{X}(x)=\\left\\{\\begin{array}{ll}{\\frac{1}{c_{L}} \\frac{1}{x^{\\alpha}}} &amp; {\\text { if } x=0,1,2, \\cdots L} \\\\ {0} &amp; {\\text { otherwise }}\\end{array}\\right. \\] where the parameter \\(c_{L}\\) is a normalization factor, and \\(\\alpha \\geq 0\\) For \\(\\alpha=1,\\) the Zipf random variable has $ E[X]=$ \\(\\text { Variance: } \\operatorname{Var}[X]=\\frac{L(L+1)}{2 c_{L}}-\\frac{L^{2}}{c_{L}^{2}}\\)","link":"/2019/09/21/Probability-and-Random-Processes/Type-of-Discrete-Random-Variable/"}],"tags":[],"categories":[{"name":"Power Electric","slug":"Power-Electric","link":"/categories/Power-Electric/"},{"name":"basic electric elements","slug":"basic-electric-elements","link":"/categories/basic-electric-elements/"},{"name":"Number Theory","slug":"Number-Theory","link":"/categories/Number-Theory/"},{"name":"Linear Control System","slug":"Linear-Control-System","link":"/categories/Linear-Control-System/"},{"name":"Probability and Random Processes","slug":"Probability-and-Random-Processes","link":"/categories/Probability-and-Random-Processes/"},{"name":"Basic","slug":"Linear-Control-System/Basic","link":"/categories/Linear-Control-System/Basic/"}]}